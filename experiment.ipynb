{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFo3KegpI3Sz",
        "outputId": "90248859-0b73-4367-f2f2-168e6b4e7dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Cifar10_ViT_ColossalAI'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 15 (delta 3), reused 15 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (15/15), 7.87 KiB | 1.97 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "/content/Cifar10_ViT_ColossalAI\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Oliverluyu/Cifar10_ViT_ColossalAI.git\n",
        "%cd Cifar10_ViT_ColossalAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Cifar10_ViT_ColossalAI/"
      ],
      "metadata": {
        "id": "OYqz_1IOXGm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!set -xe\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P8FAfibJRUh",
        "outputId": "303a68a2-572a-4338-9360-03580c6d9e98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colossalai>=0.1.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.61.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.38.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.7.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (13.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (1.11.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.6.4)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.99)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.1.12->-r requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->-r requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.1->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 5)) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 6)) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 6)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 5)) (2024.2.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: decorator>=5 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (5.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.2.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 6)) (2024.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5.35)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (20.25.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.16.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.1.2)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (42.0.5)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.1.12->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray->colossalai>=0.1.12->-r requirements.txt (line 1)) (0.18.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.1.12->-r requirements.txt (line 1)) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path for saving model\n",
        "OUTPUT_PATH=\"./output_model\"\n",
        "\n",
        "# model name or path\n",
        "MODEL=\"google/vit-base-patch16-224\"\n",
        "\n",
        "# plugin(training strategy)\n",
        "PLUGIN = \"gemini\"\n",
        "\n",
        "# configuration of parallel group sizes, only used when setting PLUGIN to \"hybrid_parallel\"\n",
        "TP_SIZE=2\n",
        "PP_SIZE=2\n",
        "\n",
        "# number of gpus to use\n",
        "GPUNUM=1\n",
        "\n",
        "# batch size per data parallel group\n",
        "BS=4\n",
        "\n",
        "# learning rate\n",
        "LR=\"2e-4\"\n",
        "\n",
        "# number of epoch\n",
        "EPOCH=10\n",
        "\n",
        "# weight decay\n",
        "WEIGHT_DECAY=0.05\n",
        "\n",
        "# ratio of warmup steps\n",
        "WARMUP_RATIO=0.3\n",
        "\n",
        "\n",
        "!colossalai run \\\n",
        "  --nproc_per_node {GPUNUM} \\\n",
        "  vit_train_demo.py \\\n",
        "  --model_name_or_path {MODEL} \\\n",
        "  --output_path {OUTPUT_PATH} \\\n",
        "  --plugin {PLUGIN} \\\n",
        "  --batch_size {BS} \\\n",
        "  --tp_size {TP_SIZE} \\\n",
        "  --pp_size {PP_SIZE} \\\n",
        "  --num_epoch {EPOCH} \\\n",
        "  --learning_rate {LR} \\\n",
        "  --weight_decay {WEIGHT_DECAY} \\\n",
        "  --warmup_ratio {WARMUP_RATIO}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vITnmsW-LmSQ",
        "outputId": "0c86e16f-8b58-4c5f-91d2-5d191e88cf71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 08:50:57.842988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 08:50:57.843171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 08:50:57.845388: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 08:50:59.630430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 08:51:01] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "Downloading readme: 100%|██████████| 4.34k/4.34k [00:00<00:00, 13.8MB/s]\n",
            "Downloading metadata: 100%|██████████| 929/929 [00:00<00:00, 4.88MB/s]\n",
            "Downloading data: 100%|██████████| 217M/217M [00:33<00:00, 6.42MB/s]\n",
            "Downloading data: 100%|██████████| 67.9M/67.9M [00:11<00:00, 5.70MB/s]\n",
            "Generating train split: 100%|██████████| 2250/2250 [00:00<00:00, 2507.31 examples/s]\n",
            "Generating test split: 100%|██████████| 750/750 [00:00<00:00, 2546.72 examples/s]\n",
            "config.json: 100%|██████████| 69.7k/69.7k [00:00<00:00, 3.28MB/s]\n",
            "model.safetensors: 100%|██████████| 346M/346M [00:11<00:00, 29.8MB/s]\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[04/12/24 08:52:44] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_train_demo.py:171 main             \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_train_demo.py:199 main             \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 38.25445294380188 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 244.7239100933075 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/12/24 08:57:29] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_train_demo.py:230 main             \n",
            "                    INFO     colossalai - colossalai - INFO: Start finetuning                       \n",
            "Epoch [1]: 100%|██████████| 562/562 [03:09<00:00,  2.96it/s, loss=0.00117]\n",
            "Evaluation result for epoch 1:                 average_loss=0.0026,                 accuracy=1.0000.\n",
            "Epoch [2]: 100%|██████████| 562/562 [03:12<00:00,  2.92it/s, loss=0.000181]\n",
            "Evaluation result for epoch 2:                 average_loss=0.0007,                 accuracy=1.0000.\n",
            "Epoch [3]: 100%|█████████▉| 561/562 [03:20<00:00,  2.82it/s, loss=4.74e-5]/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch [3]: 100%|██████████| 562/562 [03:20<00:00,  2.80it/s, loss=5.18e-5]\n",
            "Evaluation result for epoch 3:                 average_loss=0.0004,                 accuracy=1.0000.\n",
            "Epoch [4]: 100%|██████████| 562/562 [03:28<00:00,  2.70it/s, loss=2.44e-5]\n",
            "Evaluation result for epoch 4:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [5]: 100%|██████████| 562/562 [03:34<00:00,  2.62it/s, loss=1.36e-5]\n",
            "Evaluation result for epoch 5:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [6]: 100%|██████████| 562/562 [03:47<00:00,  2.47it/s, loss=9.12e-6]\n",
            "Evaluation result for epoch 6:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [7]: 100%|██████████| 562/562 [03:49<00:00,  2.45it/s, loss=6.97e-6]\n",
            "Evaluation result for epoch 7:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [8]: 100%|██████████| 562/562 [03:45<00:00,  2.49it/s, loss=5.96e-6]\n",
            "Evaluation result for epoch 8:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [9]: 100%|██████████| 562/562 [03:38<00:00,  2.57it/s, loss=5.48e-6]\n",
            "Evaluation result for epoch 9:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "Epoch [10]: 100%|██████████| 562/562 [03:38<00:00,  2.58it/s, loss=5.42e-6]\n",
            "Evaluation result for epoch 10:                 average_loss=0.0003,                 accuracy=1.0000.\n",
            "[04/12/24 09:34:26] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_train_demo.py:234 main             \n",
            "                    INFO     colossalai - colossalai - INFO: Finish finetuning                      \n",
            "[04/12/24 09:34:27] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_train_demo.py:238 main             \n",
            "                    INFO     colossalai - colossalai - INFO: Saving model checkpoint to             \n",
            "                             ./output_model                                                         \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH=\"google/vit-base-patch16-224\"\n",
        "MEMCAP=0\n",
        "GPUNUM=1\n",
        "BS = [8,32,64,128]\n",
        "\n",
        "PLUGINS = [\"torch_ddp\", \"torch_ddp_fp16\", \"low_level_zero\", \"gemini\"]\n",
        "\n",
        "for PLUGIN in PLUGINS:\n",
        "  for _BS in BS:\n",
        "    !colossalai run \\\n",
        "      --nproc_per_node {GPUNUM} \\\n",
        "      vit_benchmark.py \\\n",
        "      --model_name_or_path {MODEL_PATH} \\\n",
        "      --mem_cap {MEMCAP} \\\n",
        "      --plugin {PLUGIN} \\\n",
        "      --batch_size {_BS}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSEM6gZwrLxp",
        "outputId": "ee2e4b70-ff5c-4718-a223-22ed2ab5635f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:39:39] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:39:41] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.10216951370239258 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.12423539161682129 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:39:42] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:06<00:00,  3.02it/s]\n",
            "[04/12/24 09:39:49] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             8, plugin: torch_ddp, throughput: 24.1602, maximum memory usage per    \n",
            "                             gpu: 1.74 GB.                                                          \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:40:04] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:40:05] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09597563743591309 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11919641494750977 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:40:07] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n",
            "[04/12/24 09:40:31] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             32, plugin: torch_ddp, throughput: 26.5035, maximum memory usage per   \n",
            "                             gpu: 2.11 GB.                                                          \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:40:43] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:40:45] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.10126399993896484 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11484837532043457 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:40:46] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:49<00:00,  2.48s/it]\n",
            "[04/12/24 09:41:36] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             64, plugin: torch_ddp, throughput: 25.8283, maximum memory usage per   \n",
            "                             gpu: 2.68 GB.                                                          \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:41:48] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:41:50] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp                \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.1517934799194336 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.17877626419067383 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:41:51] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [01:37<00:00,  4.90s/it]\n",
            "[04/12/24 09:43:29] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             128, plugin: torch_ddp, throughput: 26.1302, maximum memory usage per  \n",
            "                             gpu: 4.02 GB.                                                          \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:43:43] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:43:44] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.10915350914001465 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11035728454589844 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:43:45] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:02<00:00,  6.81it/s]\n",
            "[04/12/24 09:43:48] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             8, plugin: torch_ddp_fp16, throughput: 54.4410, maximum memory usage   \n",
            "                             per gpu: 1.73 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:44:01] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:44:03] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.12072229385375977 seconds\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.17574739456176758 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:44:04] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.29it/s]\n",
            "[04/12/24 09:44:13] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             32, plugin: torch_ddp_fp16, throughput: 73.2261, maximum memory usage  \n",
            "                             per gpu: 2.03 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:44:25] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:44:27] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.1496593952178955 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.1823892593383789 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:44:29] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\n",
            "[04/12/24 09:44:45] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             64, plugin: torch_ddp_fp16, throughput: 79.5589, maximum memory usage  \n",
            "                             per gpu: 2.50 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:44:55] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:44:57] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as torch_ddp_fp16           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09681320190429688 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.12066316604614258 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:44:58] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:31<00:00,  1.56s/it]\n",
            "[04/12/24 09:45:29] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             128, plugin: torch_ddp_fp16, throughput: 82.0266, maximum memory usage \n",
            "                             per gpu: 3.64 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:45:39] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:45:40] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.1443936824798584 seconds\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.17267966270446777 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:45:42] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:04<00:00,  4.67it/s]\n",
            "[04/12/24 09:45:46] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             8, plugin: low_level_zero, throughput: 37.3072, maximum memory usage   \n",
            "                             per gpu: 1.66 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:45:59] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:46:01] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09795188903808594 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11575603485107422 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:08<00:00,  2.32it/s]\n",
            "[04/12/24 09:46:10] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             32, plugin: low_level_zero, throughput: 74.1785, maximum memory usage  \n",
            "                             per gpu: 1.67 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:46:24] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:46:26] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09486842155456543 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11220288276672363 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
            "[04/12/24 09:46:41] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             64, plugin: low_level_zero, throughput: 87.9522, maximum memory usage  \n",
            "                             per gpu: 1.88 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:46:53] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:46:56] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as low_level_zero           \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.10855555534362793 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11174297332763672 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "[04/12/24 09:46:57] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  numel += p.storage().size()\n",
            "Training Step: 100%|██████████| 20/20 [00:27<00:00,  1.36s/it]\n",
            "[04/12/24 09:47:24] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             128, plugin: low_level_zero, throughput: 94.3706, maximum memory usage \n",
            "                             per gpu: 2.59 GB.                                                      \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:47:38] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:47:40] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09286332130432129 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.1277756690979004 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/12/24 09:47:42] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step: 100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
            "[04/12/24 09:47:50] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             8, plugin: gemini, throughput: 20.5365, maximum memory usage per gpu:  \n",
            "                             663.17 MB.                                                             \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:48:03] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:48:05] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09516167640686035 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.12387895584106445 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/12/24 09:48:07] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step: 100%|██████████| 20/20 [00:10<00:00,  1.84it/s]\n",
            "[04/12/24 09:48:18] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             32, plugin: gemini, throughput: 58.9777, maximum memory usage per gpu: \n",
            "                             663.17 MB.                                                             \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:48:28] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:48:29] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09946608543395996 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11165595054626465 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/12/24 09:48:32] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
            "[04/12/24 09:48:49] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             64, plugin: gemini, throughput: 75.4652, maximum memory usage per gpu: \n",
            "                             928.37 MB.                                                             \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/pipeline/schedule/_utils.py:19: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _register_pytree_node(OrderedDict, _odict_flatten, _odict_unflatten)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'collections.OrderedDict'> is already registered as pytree node. Overwriting the previous registration.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/shardformer/layer/normalization.py:45: UserWarning: Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\n",
            "  warnings.warn(\"Please install apex from source (https://github.com/NVIDIA/apex) to use the fused layernorm kernel\")\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:48: UserWarning: `config` is deprecated and will be removed soon.\n",
            "  warnings.warn(\"`config` is deprecated and will be removed soon.\")\n",
            "[04/12/24 09:49:03] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /usr/local/lib/python3.10/dist-packages/colossalai/initialize.py:67    \n",
            "                             launch                                                                 \n",
            "                    INFO     colossalai - colossalai - INFO: Distributed environment is initialized,\n",
            "                             world size: 1                                                          \n",
            "[04/12/24 09:49:05] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:68 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Finish loading model from              \n",
            "                             google/vit-base-patch16-224                                            \n",
            "                    INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:94 main               \n",
            "                    INFO     colossalai - colossalai - INFO: Set plugin as gemini                   \n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT cpu_adam_x86 kernel during runtime now\n",
            "[extension] Time taken to compile cpu_adam_x86 op: 0.09607219696044922 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/kernel/extensions/utils.py:96: UserWarning: [extension] The CUDA version on the system (12.2) does not match with the version (12.1) torch was compiled with. The mismatch is found in the minor version. As the APIs are compatible, we will allow compilation to proceed. If you encounter any issue when using the built kernel, please try to build it again with fully matched CUDA versions\n",
            "  warnings.warn(\n",
            "[extension] Compiling the JIT fused_optim_cuda kernel during runtime now\n",
            "[extension] Time taken to compile fused_optim_cuda op: 0.11968731880187988 seconds\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/nn/optimizer/hybrid_adam.py:90: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  self._dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
            "/usr/local/lib/python3.10/dist-packages/colossalai/zero/gemini/chunk/chunk.py:45: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return tensor.storage().size() == 0\n",
            "[04/12/24 09:49:07] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:108 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Start testing                          \n",
            "Training Step: 100%|██████████| 20/20 [00:29<00:00,  1.45s/it]\n",
            "[04/12/24 09:49:36] INFO     colossalai - colossalai - INFO:                                        \n",
            "                             /content/Cifar10_ViT_ColossalAI/vit_benchmark.py:138 main              \n",
            "                    INFO     colossalai - colossalai - INFO: Testing finished, batch size per gpu:  \n",
            "                             128, plugin: gemini, throughput: 88.2502, maximum memory usage per gpu:\n",
            "                             1.61 GB.                                                               \n",
            "\n",
            "====== Training on All Nodes =====\n",
            "127.0.0.1: success\n",
            "\n",
            "====== Stopping All Nodes =====\n",
            "127.0.0.1: finish\n"
          ]
        }
      ]
    }
  ]
}